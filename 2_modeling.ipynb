{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a118f9d",
   "metadata": {},
   "source": [
    "## 딥러닝 학습\n",
    "Seq2Seq 선택\n",
    "\n",
    "+ RNN의 경우 문장이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하기 때문에, 최대 길이가 100글자가 넘는 문장을 처리하기에 부적합\n",
    "+ RNN기반인 LSTM, GRU 또한 이런 장기의존성 문제를 갖고 있음  \n",
    "    => 인코더-디코더를 이용해 장기의존성 문제를 해결한 Attention 모델 활용  \n",
    "    => Seq2Seq는 2014년 발표되어 챗봇과 기계번역에 많이 쓰이는 모델로,  \n",
    "        입력 시퀀스와 출력 시퀀스를 각각 입력 문장과 번역 문장으로 만들면 번역기를 만들 수 있을 것으로 예상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d0bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jieba\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "#!pip install sklearn\n",
    "#!pip install torch\n",
    "#가상환경에 conda install -c anaconda tensorflow-gpu==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8f1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, re, jieba\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Flatten\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa87c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2901464816232012904\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3663069184\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5149169805743902618\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#GPU 사용 가능 여부 확인 => physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability: 8.6\" 확인됨\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65d501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Running\n"
     ]
    }
   ],
   "source": [
    "# tensorflow GPU 사용방법 참고)https://github.com/tensorflow/docs-l10n/blob/master/site/ko/guide/gpu.ipynb \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU Running')\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed9477",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1ced9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(language):\n",
    "    lang_tokenizer = Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(language)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(language)\n",
    "    tensor = pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef180e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kr(w):\n",
    "    w = re.sub(r\"([?'!,¿\\-·\\\"])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[ |ㄱ-ㅎ|ㅏ-ㅣ]+', \" \", w)\n",
    "    w = w[:-1].strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w  \n",
    "\n",
    "def preprocess_ch(w):\n",
    "    w = ' '.join(jieba.cut(w, cut_all=False))   \n",
    "    w = w[:-1].rstrip().strip()   \n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed9c5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1_preprocessing.ipynb에서 만들었던 함수 활용(용량 문제 해결 위해 num_data 파라미터 추가)\n",
    "def tokenize_dataset(path, num_data):\n",
    "    files = glob.glob(os.path.join(path, '*.csv'))\n",
    "    ch, ko = [], []\n",
    "    \n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        ch.extend(df['중국어'].values)\n",
    "        ko.extend(df['한국어'].values)\n",
    "    \n",
    "    ch_series = pd.Series(ch)\n",
    "    ko_series = pd.Series(ko)\n",
    "    \n",
    "    df = pd.concat([ch_series, ko_series], axis=1)\n",
    "    df.columns = ['중국어', '한국어']\n",
    "    \n",
    "    df['중국어'] = df['중국어'].apply(preprocess_ch)\n",
    "    df['한국어'] = df['한국어'].apply(preprocess_kr)\n",
    "    \n",
    "    df = df.sample(num_data, random_state=2)\n",
    "    \n",
    "    ch_tensor, ch_tokenizer = tokenize(df['중국어'].values)\n",
    "    ko_tensor, ko_tokenizer = tokenize(df['한국어'].values)\n",
    "    \n",
    "    return ch_tensor, ko_tensor, ch_tokenizer, ko_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ebd6f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeonok\\AppData\\Local\\Temp/ipykernel_952/4236451613.py:5: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  ch_tensor, ko_tensor, ch_lang, ko_lang = tokenize_dataset(path, num_data)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yeonok\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.422 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#num_data만큼 데이터셋 크기를 제한\n",
    "num_data = 10000\n",
    "\n",
    "path = os.getcwd() + '\\\\Training'\n",
    "ch_tensor, ko_tensor, ch_lang, ko_lang = tokenize_dataset(path, num_data)\n",
    "\n",
    "#입력 텐서, 타겟 텐서의 최대 길이 계산\n",
    "print('중국어 tensor 크기 : {}'.format(ch_tensor.shape[1]))\n",
    "print('한국어 tensor 크기 : {}'.format(ko_tensor.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa142f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 최대 길이 : 중국어 3, 한국어 3\n",
      "훈련 데이터셋 크기 : 8000\n",
      "검증 데이터셋 크기 : 2000\n"
     ]
    }
   ],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_ch, max_length_ko = max_length(ch_tensor), max_length(ko_tensor)\n",
    "print('문장 최대 길이 : 중국어 {}, 한국어 {}'.format(max_length_ch, max_length_ko))\n",
    "\n",
    "ch_tensor_train, ch_tensor_val, ko_tensor_train, ko_tensor_val = train_test_split(ch_tensor, ko_tensor, test_size=0.2)\n",
    "print('훈련 데이터셋 크기 : {}'.format(len(ch_tensor_train), len(ko_tensor_train)))\n",
    "print('검증 데이터셋 크기 : {}'.format(len(ch_tensor_val), len(ko_tensor_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea47e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(tokenizer, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(\"%12d ----> %s\" % (t, tokenizer.index_word[t]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bcbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중국어 index ----> token\n",
      "           1 ----> <start>\n",
      "           4 ----> \"\n",
      "           2 ----> <end>\n",
      "\n",
      "한국어 index ----> token\n",
      "           1 ----> <start>\n",
      "           3 ----> .\n",
      "           2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print('중국어 index ----> token')\n",
    "convert(ch_lang, ch_tensor_train[0])\n",
    "print()\n",
    "print('한국어 index ----> token')\n",
    "convert(ko_lang, ko_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f7377",
   "metadata": {},
   "source": [
    "### tf.data 데이터셋 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중국어 토큰 개수 : 6\n",
      "한국어 토큰 개수 : 5\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(ch_tensor_train)\n",
    "BATCH_SIZE = 64   #Out of Memory 에러 주의\n",
    "steps_per_epoch = len(ch_tensor_train) // BATCH_SIZE\n",
    "embedding_size = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_input_size = len(ch_lang.word_index) + 1  \n",
    "vocab_target_size = len(ko_lang.word_index) + 1 \n",
    "\n",
    "print('중국어 토큰 개수 : {}'.format(vocab_input_size))\n",
    "print('한국어 토큰 개수 : {}'.format(vocab_target_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((ch_tensor_train, ko_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304120d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3) (64, 3)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "print(example_input_batch.shape, example_target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e5018",
   "metadata": {},
   "source": [
    "### 인코더 모델 생성\n",
    "+ Attention을 가진 인코더-디코더 모델을 생성\n",
    "+ input(중국어) => [batch_size, max_length_input, hidden_size]\n",
    "+ 임베딩 층(hidden state) => [batch_size, max_length_input, embedding_dim]\n",
    "+ GRU(Gated Recurrent Unit, 게이트 순환 유닛) 층 => output(한국어)[batch_size, max_length_input, enc_units],  \n",
    "                히든레이어[batch_size, enc_units]\n",
    "\n",
    "> 슈도코드 (FC=완전연결(Dense)층, EO=인코더 결과, H=은닉상태(hidden state), X=디코더에 대한 입력)\n",
    "\n",
    "+ score = FC(tanh(FC(EO) + FC(H)))로 계산하며, 형태는 [batch_size, max_length_input, hidden_size]\n",
    "+ 어텐션 가중치는 softmax(score, axis=1) \n",
    "+ 컨텍스트 벡터(context vector)는 sum(어텐션 가중치 * EO, axis = 1)\n",
    "+ 임베딩 결과(embedding output)는 디코더 X에 대한 입력이 임베딩층을 통과한 결과\n",
    "+ 병합된 벡터(merged vector)는 concat(임베딩 결과, 컨텍스트 백터(context vector))와 같음\n",
    "+ 그런 다음 병합된 벡터는 GRU에 주어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33362e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output (batch size, sequence length, units) = (64, 3, 1024)\n",
      "Encoder Hidden state  (batch size, units) = (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_input_size, embedding_size, units, BATCH_SIZE)\n",
    "\n",
    "#샘플 입력\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print(f'Encoder output (batch size, sequence length, units) = {sample_output.shape}')   \n",
    "print(f'Encoder Hidden state  (batch size, units) = {sample_hidden.shape}')            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ccb7a",
   "metadata": {},
   "source": [
    "### Attention 매커니즘\n",
    "+ output [batch_size, max_length_inp, enc_units] => values 로 사용\n",
    "+ 히든레이어 [batch_size, enc_units] => query 로 사용\n",
    "+ (참고)https://hcnoh.github.io/2018-12-11-bahdanau-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        #query hidden state는 (batch_size, hidden_size)로 구성\n",
    "        #query_with_time_axis는 (batch_size, 1, hidden_size)로 구성\n",
    "        #values는 (batch_size, max_len, hidden_size)로 구성\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        #score는 (batch_size, max_len, units)로 구성\n",
    "        #score를 self.V에 적용하기 때문에 마지막 축에 1을 얻어 (batch_size, max_len, 1)로 구성되게 됨\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "\n",
    "        #attention_weights는 (batch_size, max_len, 1)로 구성\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        #병합 이후 context_vector는 (batch_size, hidden_size)로 구성\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2429db14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(f\"Attention result shape: (batch size, units) {attention_result.shape}\")\n",
    "print(f\"Attention weights shape: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c0124",
   "metadata": {},
   "source": [
    "### 디코더 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):    # 단어 하나하나 해석 진행 \n",
    "        #            hidden (batch_size, units),    enc_output (batch_size, max_length_inp, enc_units)\n",
    "        # =>context_vector (batch_size, enc_units), attention_weights (batch_size, max_length_inp, 1)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # 임베딩 층 통과 후 x는 (batch_size, 1, embedding_dim)로 구성\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        #context vector과 임베딩 결과를 결합한 후 x는 (batch_size, 1, embedding_dim+hidden_size)로 구성\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        #위에서 결합된 벡터를 GRU에 전달\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        #output은 (batch_size*1, hidden_size)로 구성\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        #FC(완전연결층)을 지난 x는 (batch_size, vocab)으로 구성\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 5)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_target_size, embedding_size, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "\n",
    "print(f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28f104",
   "metadata": {},
   "source": [
    "### optimizer(최적화함수), 손실함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_objects = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                             reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_objects(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88fa70",
   "metadata": {},
   "source": [
    "### 모델 훈련\n",
    "1. 인코더 결과와 인코더 은닉 상태(hidden state)를 반환하는 인코더를 통해서 입력을 전달합니다.\n",
    "2. 인코더 결과, 인코더 은닉 상태(hidden state), 디코더 입력 (start 토큰)을 디코더에 전달합니다.\n",
    "3. 전달 받은 값을 통해 디코더는 예측 값과 디코더 은닉 상태(hidden state)를 반환합니다.\n",
    "4. 그 다음에 디코더 은닉 상태(hidden state)가 다시 모델에 전달되고 예측 값을 사용하여 손실을 계산합니다.\n",
    "5. 디코더에 대한 다음 입력을 결정하기 위해서 교사 강요(teacher forcing)를 사용합니다.\n",
    "6. 교사 강요(teacher forcing)는 타겟 단어가 디코더에 다음 입력으로 전달하기 위한 기술입니다.\n",
    "7. 마지막 단계는 그레디언트(gradients)를 계산하여 이를 옵티마이저(optimizer)와 역전파(backpropagate)에 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da56d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#체크포인트(객체 기반 저장)\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"cpkt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, \n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir,\n",
    "                                     checkpoint_name='model.ckpt',\n",
    "                                     max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([ko_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            #enc_output을 디코더에 전달\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden,\n",
    "                                                 enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)    #teacher forcing(다음 input으로 target을 feeding)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # 초기화할 GPU number\n",
    "\n",
    "#with tf.Graph().as_default():\n",
    "\t# GPU 메모리를 전부 할당하지 않고, 아주 적은 비율만 할당되어 시작됨. 프로세스의 메모리 수요에 따라 자동적으로 증가\n",
    "\t# 단, GPU 메모리를 처음부터 전체 비율을 사용하지 않음\n",
    "\t#gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.068760\n",
      "Epoch 1 Loss 0.0549\n",
      "Time taken for 1 epoch 4.511596918106079 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.030749\n",
      "Epoch 2 Loss 0.0055\n",
      "Time taken for 1 epoch 1.6059989929199219 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.039491\n",
      "Epoch 3 Loss 0.0060\n",
      "Time taken for 1 epoch 1.5989999771118164 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:4f}\".format(\n",
    "                epoch+1, batch, batch_loss.numpy()))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch+1, total_loss/steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time()-start))\n",
    "        \n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    manager.save()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c76ee",
   "metadata": {},
   "source": [
    "### 훈련된 모델을 사용해 번역해보기\n",
    "+ 각 마지막 시점에서 이전 디코더에서의 인코더 결과(하나의 입력에 대해 단 한번만 계산됨)와 hidden state를 가진 예측값을 디코더에 입력\n",
    "+ 모델이 \\<end\\> 토큰을 예측하는 순간 예측 중지\n",
    "+ 마지막 시점마다 attention 가중치 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_ko, max_length_ch))\n",
    "\n",
    "    sentence = preprocess_ch(sentence)\n",
    "    \n",
    "    inputs = [ch_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_ch, padding='post')\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([ko_lang.word_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_ko):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        #나중에 attention 가중치를 시각화하기 위해 저장해두기\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += ko_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if ko_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        #예측된 id를 모델에 다시 feeding\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d823c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention 가중치 그리는 함수\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print(\"Input : %s\" % (sentence))\n",
    "    print(\"Predicted translation : {}\".format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77856f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1aca41e4d60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint_dir내에 있는 최근 체크포인트 복원\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a4003",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'了'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_496/3392282120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'我迷失了'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_496/3191119241.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input : %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted translation : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_496/1705814306.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_ch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mch_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length_ch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_496/1705814306.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_ch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mch_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length_ch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '了'"
     ]
    }
   ],
   "source": [
    "translate(u'我迷失了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb799f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec72914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(train_df):\n",
    "    #글자단위 토큰화\n",
    "    ch_vocab, ko_vocab = set(), set()\n",
    "\n",
    "    for line in train_df['중국어']:\n",
    "        for c in line:\n",
    "            ch_vocab.add(c)\n",
    "\n",
    "    for line in train_df['한국어']:\n",
    "        for c in line:\n",
    "            ko_vocab.add(c)\n",
    "            \n",
    "    ch_vocab_size = len(ch_vocab) + 1  #94\n",
    "    ko_vocab_size = len(ko_vocab) + 1  #4837\n",
    "    \n",
    "    #set -> list(데이터 변경 용이한 자료구조로 변환)\n",
    "    ch_vocab = sorted(list(ch_vocab))\n",
    "    ko_vocab = sorted(list(ko_vocab))\n",
    "    \n",
    "    ch_to_index = dict([(c, i+1) for i, c in enumerate(ch_vocab)])\n",
    "    ko_to_index = dict([(c, i+1) for i, c in enumerate(ko_vocab)])\n",
    "    \n",
    "    #중국어 문장 인코딩\n",
    "    encoder_input = []\n",
    "    for li in train_df['중국어']:\n",
    "        t = []\n",
    "        for c in li:\n",
    "            t.append(ch_to_index[c])\n",
    "        encoder_input.append(t)\n",
    "        \n",
    "    #한국어 문장 인코딩\n",
    "    decoder_input = []\n",
    "    for li in train_df['한국어']:\n",
    "        t = []\n",
    "        for c in li:\n",
    "            t.append(ko_to_index[c])\n",
    "        decoder_input.append(t)   \n",
    "        \n",
    "    #번역되어 나올 한국어 문장 인코딩에서 '\\t' 제거\n",
    "    decoder_ko = []\n",
    "    for li in train_df['한국어']:\n",
    "        t = []\n",
    "        i = 0\n",
    "        for c in li:\n",
    "            if i > 0:\n",
    "                t.append(ko_to_index[c])\n",
    "            i += 1\n",
    "        decoder_ko.append(t)    \n",
    "     \n",
    "    #패딩\n",
    "    max_len_ch = 1689\n",
    "    max_len_ko = 373\n",
    "    \n",
    "    #문장 -> int -> padding\n",
    "    encoder_input = pad_sequences(encoder_input, maxlen=max_len_ch, padding='post')\n",
    "    decoder_input = pad_sequences(decoder_input, maxlen=max_len_ko, padding='post')\n",
    "    decoder_ko = pad_sequences(decoder_ko, maxlen=max_len_ko, padding='post') \n",
    "    \n",
    "    #문장들을 3차원 배열로 변환 : (encoder_input, decoder_input, decoder_target)\n",
    "    #encoder_input은 (문장 개수, 문장 최대 길이, 문자 종류 수) 형태의 3차원 배열로 중국어 문장의 one-hot 형식 벡터 데이터\n",
    "    #decoder_input은 (문장 개수, 문장 최대 길이, 문자 종류 수) 형태의 3차원 배열로 한국어 문장의 one-hot 형식 벡터 데이터\n",
    "    #decoder_ko은 decoder_input과 같지만, 하나의 time step만큼 offset, 즉, decoder_target[:, t, :] = decoder_input[:, t+1, :]\n",
    "    encoder_input = np_utils.to_categorical(encoder_input)\n",
    "    decoder_input = np_utils.to_categorical(decoder_input)\n",
    "    decoder_ko = np_utils.to_categorical(decoder_ko)\n",
    "    \n",
    "    return encoder_input, decoder_input, decoder_ko, ch_vocab_size, ko_vocab_size, index_to_ch, index_to_ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c4d62",
   "metadata": {},
   "source": [
    "#### 데이터가 너무 많으면 한번에 토큰화할 수 없기 때문에,\n",
    "#### 데이터를 4000개씩 나누어 토큰화 > 모델학습 > 저장 > 전이학습 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df[:4000]\n",
    "    \n",
    "encoder_input, decoder_input, decoder_ko, ch_vocab_size, ko_vocab_size, index_to_ch, index_to_ko = tokenize(df)\n",
    "\n",
    "#중국어 인코딩\n",
    "tmp_dict = dict((i,c ) for c , i in index_to_ch.items()) \n",
    "\n",
    "for i in tmp_dict:\n",
    "    try:\n",
    "        tmp_dict[i] = tmp_dict[i].encode('EUC_CN')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "index_to_ch = dict((i,c ) for c , i in tmp_dict.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이닝 시 이전 상태의 실제값을 현재상태의 디코더 입력으로 해야함(예측값으로 하면 안됨)\n",
    "encoder_inputs = Input(shape=(None, ch_vocab_size), name='encoder_input')\n",
    "decoder_inputs = Input(shape=(None, ko_vocab_size ), name='decoder_input')\n",
    "\n",
    "# 인코더 LSTM 셀\n",
    "encoderLSTM = LSTM(units=256, return_state=True, name='encoderLSTM')    #return_state :인코더의 마지막 상태 정보를 디코더의 입력 상태 정보로 전달\n",
    "decoderLSTM = LSTM(units=256, return_sequences=True, return_state=True, name='decoderLSTM')\n",
    "\n",
    "# 인코더 LSTM셀의 입력 정의\n",
    "encoder_outputs, stateH, stateC = encoderLSTM(encoder_inputs) # _, 히든상태(위), 셀상태(오른쪽)\n",
    "encoder_state = [stateH, stateC] # 컨텍스트 벡터\n",
    "\n",
    "decoder_output, _, _ = decoderLSTM(decoder_inputs, initial_state=encoder_state)\n",
    "decoder_softmax = Dense(ko_vocab_size, activation=\"softmax\")\n",
    "decoder_output = decoder_softmax(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.fit(x=[encoder_input,decoder_input], y=decoder_ko, batch_size=64, epochs=50, callbacks=early_stopping)\n",
    "save_model(model, 'ch_to_ko.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c02352",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(train_df) // 2500):\n",
    "    df = train_df[i*(2500):(i+1)*2500]\n",
    "    \n",
    "    encoder_input, decoder_input, decoder_ko, ch_vocab_size, ko_vocab_size = tokenize(df)\n",
    "\n",
    "    model = load_model('ch_to_ko.h5')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    model.fit(x=[encoder_input, decoder_input], y=decoder_ko, batch_size=64, epochs=3, callbacks=early_stopping)\n",
    "    save_model(model, 'ch_to_ko.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_state)\n",
    "\n",
    "ch_to_index = dict((i,c ) for c , i in index_to_ch.items()) \n",
    "ko_to_index = dict((i,c ) for c , i in index_to_ko.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02382d23",
   "metadata": {},
   "source": [
    "## 기본 LSTM 기반의 seq2seq 모델을 이용해 decoder_ko 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e136bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_state_input_hidden = Input(shape=(256,))\n",
    "decoder_state_input_cell = Input(shape=(256,))\n",
    "decoder_state_input = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "\n",
    "decoder_output, state_hidden, state_cell = decoderLSTM(decoder_inputs, initial_state = decoder_state_input)\n",
    "decoder_state = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_softmax(decoder_output)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_inputs]+decoder_state_input, outputs=[decoder_output]+decoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19349d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq): \n",
    "    \n",
    "    state_value = encoder_model.predict(input_seq)\n",
    "    print('encoder_model의 예상 state_value :',np.shape(state_value))\n",
    "    \n",
    "    target_seq = np.zeros((1,1,ko_vocab_size))   #(1, 1, 1134)\n",
    "    target_seq[0,0,ko_to_index['\\t']] = 1      # 원핫인코딩\n",
    "    \n",
    "    stop = False\n",
    "    decoded_sent=\"\"\n",
    "    while not stop: # \"\\n\"문자를 만날때까지 반복\n",
    "        \n",
    "        output, h, c = decoder_model.predict([target_seq]+state_value)\n",
    "        # 예측값을 한국어 문자로 변환\n",
    "        token_index = np.argmax(output[0,-1,:]) \n",
    "        pred_char = index_to_ko[token_index]\n",
    "        \n",
    "        # 현시점 예측문자가 예측문장에 추가\n",
    "        decoded_sent += pred_char\n",
    "        \n",
    "        if (pred_char == \"\\n\" or len(decoded_sent) > 373):\n",
    "            stop = True\n",
    "            \n",
    "        # 현시점 예측결과가 다음 시점에 입력으로 \n",
    "        target_seq = np.zeros((1,1,ko_vocab_size))\n",
    "        target_seq[0,0,token_index] = 1\n",
    "        \n",
    "        # 현시점 상태를 다음 시점 상태로 사용\n",
    "        state_value = [h,c]\n",
    "    \n",
    "    return decoded_sent # 번역결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7419ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [1,50,100,200,300]:\n",
    "    \n",
    "    input_seq = encoder_input[seq_index:seq_index+1]    # (1, 117, 2326)\n",
    "    decoded_seq = decode_seq(input_seq)\n",
    "    \n",
    "    print(\"입력문장:\", train_df['중국어'][seq_index])\n",
    "    print(\"정답:\", train_df['한국어'][seq_index][1:len(train_df['한국어'][seq_index])-1])   # \"\\t\", \"\\n\" 제거\n",
    "    print(\"번역기:\", decoded_seq[:len(decoded_seq)-1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d8038",
   "metadata": {},
   "source": [
    "## 모델이 잘 작동하는지 확인하기 위해 일부 문장 디코딩\n",
    "    -encoder_input을 샘플링해 decoder_target으로 변환해본다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb88e109d514f8fe4882139470fc892eb0cca4de07a5bc3f5da4dc1ae8fba1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
