# 딥러닝을 이용한 중국어 번역모델 만들기

## 프로젝트 기획 배경  
대학교에서 중국학부를 전공하면서 중국어의 난이도를 체감했습니다.  
중국어의 한자는 총 약 3만 6천개로, 통용되는 한자는 약 7천개이며, 자주 사용되는 한자만 추려도 약 3천개 이상입니다.  
특히 한자문화권이 아닌 국가에서는 세계에서 가장 배우기 어려운 언어 중 하나로 꼽히기도 합니다.  
아래는 영국 외무부에서 각 나라의 영국 대사관 직원들에게 현지 언어를 배우도록 한 뒤  
설문조사 한 데이터를 모아 세계 언어의 난이도를 다섯 등급으로 나눈 표인데,  
아랍어, 광동어, 표준중국어, 일본어, 한국어가 가장 배우기 어려운 등급으로 분류되어 있습니다.
![image](https://user-images.githubusercontent.com/88722429/175809275-88747d13-e93b-41d7-b60b-35856b7ab3b9.png)  

아래의 데이터 샘플을 살펴보면, 한국어는 단어별로 띄어쓰기, 품사를 나타내는 조사 등  
읽거나 번역하는 데에 힌트가 될 요소들이 있지만, 중국어는 그렇지 않습니다. 
그래서 글이 길어질수록 해석 난이도는 급격히 올라갑니다.
![image](https://user-images.githubusercontent.com/88722429/175809417-3d9566f9-d21a-424e-b7a4-f9631438752d.png)  

> 하지만 인간이 꾸준히 공부하듯이 대량의, 양질의 데이터를 딥러닝모델에 훈련시킨다면?

## 데이터
+ [AI HUB 한국어-중국어 번역 말뭉치 데이터(사회과학)](https://aihub.or.kr/aidata/30721)
+ [AI HUB 한국어-중국어 번역 말뭉치 데이터(기술과학)](https://aihub.or.kr/aidata/30722)  
각각 104만개의 데이터 => 총 208만개

## 모델
Seq2Seq 선택
+ RNN의 경우 문장이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하기 때문에, 최대 길이가 100글자가 넘는 문장을 처리하기에 부적합
+ RNN기반인 LSTM, GRU 또한 이런 장기의존성 문제를 갖고 있음  
  => 인코더-디코더를 이용해 장기의존성 문제를 해결한 Attention 모델 활용 
  => Seq2Seq는 2014년 발표되어 챗봇과 기계번역에 많이 쓰이는 모델로,   
    입력 시퀀스와 출력 시퀀스를 각각 입력 문장과 번역 문장으로 만들면 번역기를 만들 수 있을 것으로 예상

      
## 더 진행해야 할 점
[] 학습된 모델로 번역 결과 출력
[] transformer 아키텍처 기반 모델과 비교
